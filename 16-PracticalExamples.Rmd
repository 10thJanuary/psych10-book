---
output:
  pdf_document: default
  html_document: default
---
# The process of statistical modeling: A practical example

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(BayesFactor)
library(emmeans)
library(brms)

set.seed(123456) # set random seed to exactly replicate results

# load the NHANES data library
library(NHANES)

# drop duplicated IDs within the NHANES dataset
NHANES=NHANES %>% dplyr::distinct(ID,.keep_all=TRUE)

NHANES_adult = NHANES %>%
  subset(Age>=18)

```

In this chapter we will bring together everything that we have learned to apply our knowledge to a practical example.

## The process of statistical modeling

There is a set of steps that we generally go through when we want to use our statistical model to test a scientific hypothesis:

1. Specify your question of interest
2. Identify or collect the appropriate data
3. Prepare the data for analysis
4. Determine the appropriate model
5. Fit the model to the data
6. Criticize the model to make sure it fits properly
7. Test hypothesis and quantify effect size

Let's look at a real example.  In 2007, Christopher Gardner and colleagues from Stanford published a study in the *Journal of the American Medical Association* titled "Comparison of the Atkins, Zone, Ornish, and LEARN Diets for Change in Weight and Related Risk Factors Among Overweight Premenopausal Women
The A TO Z Weight Loss Study: A Randomized Trial". 

### 1: Specify your question of interest

According to the authors, the goal of their study was:

> To compare 4 weight-loss diets representing a spectrum of low to high carbohydrate intake for effects on weight loss and related metabolic variables.

### 2: Identify or collect the appropriate data

To answer their question, the investigators randomly assigned each of 311 overweight/obese women to one of four different diets (Atkins, Zone, Ornish, or LEARN), and followed their weight loss and other measures of health over time.  

The authors recorded a large number of variables, but for the main question of interest let's focus on a single variable: Body Mass Index (BMI).  Further, since our goal is to measure lasting changes in BMI, we will only look at the measurement taken at 12 months after onset of the diet.

### 3: Prepare the data for analysis

The actual data from the A to Z study are not publicly available, so we will use the summary data reported in their paper to generate some synthetic data that roughly match the data obtained in their study.

```{r}

set.seed(123456)
# generate a dataset based on the results of Gardner et al. Table 3
dietDf = data.frame(diet=c(rep('Atkins',77),rep('Zone',79),
                           rep('LEARN',79),rep('Ornish',76))) %>%
  mutate(BMIChange12Months=ifelse(diet=='Atkins',rnorm(n=77,mean=-1.65,sd=2.54),
                                  ifelse(diet=='Zone',rnorm(n=79,mean=-0.53,sd=2.0),
                                  ifelse(diet=='LEARN',rnorm(n=79,mean=-0.92,sd=2.0),
                                         rnorm(n=76,mean=-0.77,sd=2.14) ))),
         physicalActivity=ifelse(diet=='Atkins',rnorm(n=77,mean=34,sd=6),
                                  ifelse(diet=='Zone',rnorm(n=79,mean=34,sd=6.0),
                                  ifelse(diet=='LEARN',rnorm(n=79,mean=34,sd=5.0),
                                         rnorm(n=76,mean=35,sd=7) ))))
summaryDf=dietDf %>% 
  group_by(diet) %>% 
  summarize(n=n(),
            meanBMIChange12Months=mean(BMIChange12Months),
            varBMIChange12Months=var(BMIChange12Months)) %>%
  mutate(crit_val_lower = qt(.05, n - 1),
         crit_val_upper = qt(.95, n - 1),
         ci.lower=meanBMIChange12Months+(sqrt(varBMIChange12Months)*crit_val_lower)/sqrt(n),
         ci.upper=meanBMIChange12Months+(sqrt(varBMIChange12Months)*crit_val_upper)/sqrt(n))
summaryDf


```

Now that we have the data, let's visualize them to make sure that there are no outliers.  First, we will look at box plots for each condition (Figure \@ref(fig:AtoZBMIChange)).  
```{r AtoZBMIChange,fig.cap="A box plot showing the distribution of change in BMI for each diet group."}
ggplot(summaryDf,aes(x=diet,y=meanBMIChange12Months)) +
  geom_point(size=2) + 
  geom_errorbar(aes(ymin = ci.lower, ymax = ci.upper), width = 0, size = 1) +
  ylab('mean BMI change over 12 months (+/- 95% CI)')


```
That looks pretty reasonable - in particular, there are no big outliers. Violin plots are also useful to see the shape of the distributions, as shown in Figure \@ref(fig:AtoZBMIChangeDensity).

```{r AtoZBMIChangeDensity,fig.cap="Violin plots for each condition, with the 50th percentile (i.e the median) shown as a black line for each group."}
ggplot(dietDf,aes(diet,BMIChange12Months)) + 
  geom_violin(draw_quantiles=.5)

```
One thing this shows us is that the distributions seem to vary a bit in the variance, with Atkins and Ornish showing greater variability than the others.  This means that any analyses that assume the variances are equal across groups could be inappropriate.

### 4. Determine the appropriate model

There are several questions that we need to ask in order to determine the appropriate statistical model for our analysis.

- What kind of dependent variable?
  * BMI : continuous, ~normally distributed
- What are we comparing?
  * mean BMI across four diet groups
  * ANOVA is appropriate
- Are observations independent?
  * random assignment and use of difference scores should ensure that IID assumption is appropriate

### 5. Fit the model to the data

Let's run an ANOVA on BMI change to compare it across the four diets. It turns out that we don't actually need to generate the dummy-coded variables ourselves; if we give lm() a categorical variable, it will automatically generate them for us.

```{r}
lmResult=lm(BMIChange12Months ~ diet, data = dietDf)
lmResult
```
Note that lm automatically generated dummy variables that correspond to three of the four diets, leaving the Atkins diet without a dummy variable.  This means that the intercept models the Atkins diet, and the other three variable model the difference between each of the those diets and the Atkins diet.

### 6. Criticize the model to make sure it fits properly

The first thing we want to do is to critique the model to make sure that it is appropriate. One thing we can do is to look at the residuals from the model. In this case, we will plot the residuals for each individual grouped by diet. We will jitter the points so that we can see all of them.

```{r}
ggplot(data.frame(residuals=lmResult$residuals,diet=dietDf$diet),aes(x=diet,y=residuals)) +
  geom_point(position=position_jitter(.1))

```
There are no obvious differences in the residuals across conditions, suggesting that we can move forward and interpret the model outputs.

### 7. Test hypothesis and quantify effect size

First let's look at the summary of results from the ANOVA:

```{r}
summary(lmResult)
```

The significant F test shows us that there is a significant difference between diets, but we should also note that the model doesn't actually account for much variance in the data; the R-squared value is only 0.03, showing that the model is only accounting for a few percent of the variance in weight loss.  Thus, we would not want to overinterpret this result.

The significant result also doesn't tell us which diets differ from which others. 
We can find out more by comparing means across conditions using the ```emmeans()``` ("estimated marginal means") function:

```{r}
# compute the differences between each of the means
leastsquare = emmeans(lmResult, 
                      pairwise ~ diet,
                      adjust="tukey")
 
# display the results by grouping using letters

CLD(leastsquare$emmeans, 
    alpha=.05,  
    Letters=letters)

```

The letters in the rightmost column show us which of the groups differ from noe another according to a method that adjusts for the number of comparisons being performed.  This shows that Atkins and LEARN diets don't differ from one another (since they share the letter a), and the LEARN, Ornish, and Zone diets don't differ from one another (since they share the letter b), but the Atkins diet differs from all of the others (since they share no letters).

#### Bayes factor

Let's say that we want to have a better way to describe the amount of evidence provided by the data.  One way we can do this is to compute a Bayes factor, which we can do by fitting the full model (including diet) and the reduced model (without diet) and the compare their fit. For the reduced model, we just include a 1, which tells the fitting program to only fit an intercept.  

```{r results='hide',message=FALSE}
brmFullModel=brm(BMIChange12Months ~ diet, data = dietDf,save_all_pars = TRUE)
brmReducedModel=brm(BMIChange12Months ~ 1, data = dietDf,save_all_pars = TRUE)
```

```{r}
bayes_factor(brmFullModel,brmReducedModel)
```

This shows us that there is very strong evidence (Bayes factor of nearly 100) for differences between the diets.

### What about possible confounds?

If we look more closely at the Garder paper, we will see that they also report statistics on how many individuals in each group had been diagnosed with *metabolic syndrome*, which is a syndrome characterized by high blood pressure, high blood glucose, excess body fat around the waist, and abnormal cholesterol levels and is associated with increased risk for cardiovascular problems. Let's first add those data into the summary data frame:

```{r}
summaryDf=summaryDf %>% 
                    mutate(nMetSym=c(22,20,29,27),
                           nNoMetSym=n-nMetSym)
```

Let's say that we are interested in testing whether the rate of metabolic syndrome was significantly different between the groups, since this might make us concerned that these differences could have affected the results of the diet outcomes. 

#### Determine the appropriate model

- What kind of dependent variable?
  * proportions
- What are we comparing?
  * proportion with metabolic syndrome across four diet groups
  * chi-squared test for goodness of fit is appropriate against null hypothesis of no difference

Let's compute that statistic using the ```chisq.test()``` function:

```{r}
chisq.test(summaryDf$nMetSym,summaryDf$nNoMetSym)
```

This test shows that there is not a significant difference between means. However, it doesn't tell us how certain we are that there is no difference; remember that under NHST, we are always working under the assumption that the null is true unless the data show us enough evidence to cause us to reject this null hypothesis.

What if we want to quantify the evidence for or against the null?  We can do this using the Bayes factor.

```{r}

bf = contingencyTableBF(as.matrix(summaryDf[,9:10]), sampleType = "indepMulti", fixedMargin = "cols")
bf
```

This shows us that the alternative hypothesis is 0.058 times more likely than the null hypothesis, which means that the null hypothesis is 1/0.058 ~ 17 times more likely than the alternative hypothesis given these data. This is fairly strong, if not completely overwhelming, evidence.

