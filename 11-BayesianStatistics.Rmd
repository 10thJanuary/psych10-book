---
output:
  pdf_document: default
  html_document: default
---
# Bayesian statistics


```{r echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(cowplot)
library(boot)
library(MASS)
library(BayesFactor)
set.seed(123456) # set random seed to exactly replicate results

# load the NHANES data library
library(NHANES)

# drop duplicated IDs within the NHANES dataset
NHANES <- 
  NHANES %>% 
  dplyr::distinct(ID,.keep_all=TRUE)

NHANES_adult <-
  NHANES %>%
  drop_na(Weight) %>%
  subset(Age>=18)

```

In this chapter we will take up the approach to statistical modeleing and inference that stands in contrast to the null hypothesis testing framework that you encountered in the previous chapter.  This is known as "Bayesian statistics" after the Reverend Thomas Bayes, whose theorem you have already encountered.  In this chapter you will learn how Bayes' theorem provides a way of understanding data that solves many of the problems that we discussed regarding null hypothesis testing.

## Generative models

Say you are walking down the street and a friend of yours walks right by but doesn't say hello.  You would probably try to decide why this happened -- Did they not see you?  Are they mad at you?  Are you suddenly cloaked in a magic invisibility shield?  One of the basic ideas behind Bayesian statistics is that we want to infer the details of how the data are being generated, based on the data themselves.  In this case, you want to use the data (i.e. the fact that your friend did not say hello) to infer the process that generated the data (e.g. whether or not they actually saw you, how they feel about you, etc).  

The idea behind a generative model is that we observe data that are generated by a *latent* (unseen) process, usually with some amount of randomness in the process.  In fact, when we take a sample of data from a population and estimate a parameter from the sample, what we are doing in essence is trying to learn the value of a latent variable (the population mean) that gives rise through sampling to the observed data (the sample mean).

If we know the value of the latent variable, then it's easy to reconstruct what the observed data should look like.  For example, let's say that we are flipping a coin that we know to be fair.  We can describe the coin by a binomial distribution with a value of p=0.5, and then we could generate random samples from such a distribution in order to see what the observed data should look like. However, in general we are in the opposite situation: We don't know the value of the latent variable of interest, but we have some data that we would like to use to estimate it. 

## Bayes' theorem and inverse inference

The reason that Bayesian statistics has its name is because it takes advantage of Bayes' theorem to make inferences from data back to some features of the (latent) model that generated the data.  
*CHANGE EXAMPLE*
Let's say that we want to know whether a coin is fair.  To test this, we flip the coin 10 times and come up with 7 heads.  Before this we were pretty sure that the coin was fair (i.e., that $p_{heads}=0.5$), but these data would certainly give us pause.  We already know how to compute the conditional probability that we would flip 95 or more heads if the coin is really fair ($P(n\ge95|p_{heads}=0.5)$), using the binomial distribution:

```{r}
pbinom(95,100,.5,lower.tail=FALSE)
```

That is an exceedingly small number.  However, this number doesn't really answer the question that we are asking -- it is telling us about the likelihood of the data given some parameter, whereas what we really want to know is the parameter value. This should sound familiar, as it's exactly the situation that we were in with null hypothesis testing, which told us about the likelihood of data rather than the likelihood of hypotheses.

Remember that Bayes' theorem provides us with the tool that we need to invert a conditional probability:

$$
P(H|D) = \frac{P(D|H)*P(H)}{P(D)}
$$

We can think of this theorem as having four parts:

- prior ($P(H)$): Our degree of belief about hypothesis H before seeing the data D
- likelihood ($P(D|H)$): How likely are the observed data D under hypothesis H?
- marginal likelihood ($P(D)$): How likely are the observed data, collapsing over all possible hypotheses?
- posterior ($P(H|D)$): Our updated belief about hypothesis H, given the data D

Here we see one of the primary differences between frequentist and Bayesian statsistics.  Frequentists do not beleive in the idea of a probability of a hypothesis -- for them, a hypothesis is either true or it isn't.  Another way to say this is that for the frequentist, the hypothesis is fixed and the data are random, which is why frequentist inference focused on describing the probability of data given a hypothesis (i.e. the p-value).  Bayesians, on the other hand, are comfortable making probability statements about both data and hypotheses.

## Doing Bayesian estimation

We ultimately want to use Bayesian statistics to test hypotheses, but before we do that we need to estimate the parameters that are necessary to test the hypothesis. Here we will walk through the process of Bayesian estimation.  Let's say that we want to test whether a coin is fair (i.e. $p_{heads}=0.5$).  To begin with, we have no prior knowledge about the coin, so we have no reason to believe that it is fair or not; in probability terms, we have no reason to think that any particular value of $p_{heads}$ is more likely than any other.  Let's walk through how to do this using Bayes' theorem.

#### from MH: Careful: You are saying several potentially different things and I think you're going to then say this corresponds to the uniform prior. If we conceive of the prior as saying we are uncertain whether or not the coin is fair, this could look like a bernoulli (flip) which then leads to a fork: either it is fair (p = 0.5) or it is biased (p ~ uniform(0,1) )

### Specifying the prior

To use Bayes' theorem, we first need to specify the prior probability for the hypothesis.  In this case, we didn't have any reason to think that one value was more likely than another, so we can use the *uniform distribution* as our prior, since all values are equally likely under a uniform distribution (see Figure \@ref(fig:uniformDist)).  For this example, we will just enumerate a set of possible values of $p_{heads}$, from 0.01 to 0.99 in steps of 0.01.

```{r uniformDist,fig.cap="Density of a uniform distribution is constant for all values. The blue line denotes the hypothesis of p(heads)=0.5."}

# we need to scale the uniform by the number of possible values. let's use 
# steps of 0.01
uniformDf <- 
  tibble(
    steps = seq(.01,.99,.01)
  ) %>%
  mutate(
    # create a prior that reflects a uniform distribution across all steps
    prior = dunif(steps)/length(steps)
  )

p <- ggplot(uniformDf,aes(steps,prior)) +
  geom_line() + 
  xlab('value') + ylab('density') +
  ylim(0,0.02) + 
  geom_vline(xintercept = 0.5,color='blue')

print(p)
```

## Collect some data

We need some data in order to estimate how likely the coin is to come up heads, so that we can test our hypothesis that it is fair. Let's say that we flip the coin 10 times, and it comes up heads 7 times.  

#### from MH: I think this whole example can be made into an interesting story. of course, I am biased by the way I teach this stuff, but for two examples, see the aside on Spinning Coins here: http://probmods.org/chapters/bayesian-data-analysis.html and the early part of this chapter: https://mhtess.github.io/bdappl/chapters/03-simpleModels.html

## Computing the likelihood

We want to compute the likelihood of the data under the hypothesis that the coin is fair (i.e. $P(n=7|p_{heads}=0.5)$), which we can do using the ```dbinom``` function in R:

```{r}
likelihood <- dbinom(7,10,.5)
likelihood

```

Let's plot the observed data against the entire distribution under the hypothesis of $p_{heads}=0.5$ (see Figure \@ref(fig:like1)):

```{r like1,fig.cap='Likelihood of each possible values of n under hypothesis of p(heads)=0.5.  Observed value shown blue.'}

likeDf <- 
  tibble(
    heads = seq(1,10,1),
    likelihood5 = dbinom(seq(1,10,1),10,.5)
  )

ggplot(likeDf,aes(heads,likelihood5)) + 
  geom_line(color='red') +
  xlab('number of heads') + ylab('probability') +
  geom_vline(xintercept = 7,color='blue')
```

We can also look at the likelihoods for other possible hypotheses (see Figure \@ref(fig:like2)) in relation to our data.  Looking at this, it seems that our observed data are relatively more likely under the hypothesis of $p_{heads}=0.7$, somewhat less likely under the hypothesis of $p_{heads}=0.5$, and quite unlikely under the hypothesis of $p_{heads}=0.3$.  One of the fundamental ideas of Bayesian inference is that we will try to find the value of $p_{heads}$ that makes the data most likely, while also taking into account our prior knowledge.

```{r like2,fig.cap='Likelihood of each possible values of n under several different hypotheses (p(heads)=0.5 (red), 0.7 (green), 0.3 (black).  Observed value shown blue.'}

likeDf <- 
  likeDf %>%
  mutate(likelihood7 = dbinom(seq(1,10,1),10,.7),
         likelihood3 = dbinom(seq(1,10,1),10,.3)
  )

ggplot(likeDf,aes(heads,likelihood5)) + 
  geom_line(color='red') +
  xlab('number of heads') + ylab('probability') +
  geom_vline(xintercept = 7,color='blue') +
  geom_line(aes(heads,likelihood7),color='green') +
  geom_line(aes(heads,likelihood3),color='black')


```

## Computing the marginal likelihood

We also need to know the overall likelihood of flipping 7 coins on 10 tries; this *marginal likelihood* is primarily important because it normalizes the other values, ensures that the result is a true probability. Using R we can compute this as follows, by dividing the number of ways in which one could choose 7 out of 10 by the total number of possible outcomes across 10 flips (which is $2^{10}$):

#### from MH: This has got me thinking that it may best pedagogically to have a simple example with two or three discrete hypotheses (rather than a continuum of coin weights) and do the math in a table or tree form. At some point, you would need to do a marginal likelihood calculation, and so at this point, you could refer back to that step in the procedure.

```{r}
marginal_likelihood <- choose(10,7)/2**10
marginal_likelihood
```

Note that this is actually exactly the same as $P(n=7|p_{heads}=0.5)$.

### Computing the posterior

We now have all of the parts that we need to compute the posterior probability of of 7 out of 10 heads, given the uniform prior:

```{r}
likelihood <- dbinom(7,10,.5)
prior <- 1/10
posterior <- (likelihood*prior)/marginal_likelihood
posterior

```

This tells us that the hypothesis of a fair coin is fairly unlikely given our observed data; in fact, it's slightly lower than it was before we saw the data (i.e. the prior).

We can also look at the entire distribution of posterior values, which tells us how likely each possible value of $p_{heads}$ is given the data (see Figure \@ref(fig:posteriorDist)).

```{r posteriorDist,fig.cap="Posterior distribution plotted in blue against uniform prior distribution (dotted black line)."}

# compute likelihoods for the observed data under all values of p(heads).  here we use the quantized values from .01 to .99 in steps of 0.01

bayesDf <-
  tibble(
    steps = seq(0.01,0.99,0.01)
  ) %>%
  mutate(
    likelihoods = dbinom(7,10,steps),
    priors = dunif(steps)/length(steps),
    posteriors = (likelihoods*priors)/marginal_likelihood
 )

ggplot(bayesDf,aes(steps,posteriors)) +
  geom_line(color='blue') +
  geom_line(aes(steps,priors),color='black',linetype='dotted') +
  xlab('p(heads)') + ylab('Probability')

```

### Maximum a posteriori (MAP) estimation

Given our data we might also like to obtain an estimate of p(heads) for our coin.  One way to do this is to find the value of p(heads) for which the posterior probability is the highest, which we refer to as the *maximum a posteriori* (MAP) estimate.  We can find this from the data in \@ref(fig:posteriorDist):

```{r}

MAP_estimate <- bayesDf$steps[which.max(bayesDf$posteriors)]
MAP_estimate
```

### Credible intervals

Often we would like to know not just a single estimate for the posterior, but an interval in which we are confident that the posterior falls.  We previously discussed the concept of confidence intervals in the context of frequentist inference, and you may remember that the interpretation of confidence intervals was particularly convoluted.  What we really want is an interval in which we are confident that the true parameter falls, and Bayesian statistics can give us such an interval, which we call a *credible interval*.

In some cases the credible interval can be computed *numerically* based on a known distribution, but it's more common to generate a credible interval by sampling from the posterior distribution and then compute quantiles of the samples. This is particularly useful when we don't have an easy way to express the posterior distribution numerically, which is often the case in real Bayesian data analysis (as we will see later).  

We will generate samples from our posterior distribution using a simple algorithm known as [*rejection sampling*](https://am207.github.io/2017/wiki/rejectionsampling.html).  The idea is that we choose a random value of x (in this case $p_{heads}$) and a random value of y (in this case, the posterior probability of $p_{heads}$) each from a uniform distribution, and we accept the sample only if $y < f(x)$ - in this case, if the randomly selected value of y is less than the actual posterior probability of y.  Figure \@ref(fig:rejectionSampling) shows an example of a histogram of samples using rejection sampling, along with the 95% credible intervals obtained using this method.  

```{r rejectionSampling,fig.cap="Rejection sampling example.The black line shows the density of all possible values of p(heads); the blue lines show the 2.5th and 97.5th percentiles of the distribution, which represent the 95% credible interval for the estimate of p(heads)."}

nsamples <- 100000

# create random uniform variates for x and y
x <- runif(nsamples)
y <- runif(nsamples)

# create f(x)
fx <- dbinom(7,10,x)

# accept samples where y < f(x)
accept <- which(y<fx)
accepted_samples <- x[accept]

credible_interval <- quantile(accepted_samples,c(0.025,0.975))
credible_interval

# plot histogram

p=ggplot(data.frame(samples=accepted_samples),aes(samples)) + 
  geom_density()

for (i in 1:2) {
  p = p + annotate('segment',x=credible_interval[i],xend=credible_interval[i],
           y=0,yend=2,col='blue',lwd=1) 
} 
print(p)
```

The interpretation of this credible interval is much closer to what we had hoped we could get from a confidence interval (but could not): It tells us that there is a 95% probability that the value of $p_{heads}$ falls between these two values.

### Effects of different priors

In the previous example we used a *flat prior*, meaning that we didn't have any reason to believe that any particular value of $p_{heads}$ was more or less likely.  However, let's say that we had instead started with some previous data: We had flipped 10 coins, and 5 of them came up heads.  This would have lead us to start with a prior belief that the coin was fair.  We can do the same computation as above, but using the information about our previous coin flips as the prior (see Figure \@ref(fig:posteriorDistPrior)).  


```{r posteriorDistPrior,fig.cap="Effects of priors on the posterior distribution.  The original posterior distribution based on a flat prior is plotted in blue. The prior based on the observation of 5 heads out of 10 flips is plotted in the dotted black line, and the posterior using this prior is plotted in red."}

# compute likelihoods for data under all values of p(heads).  here we use the quantized values from .01 to .99 in steps of 0.01

df <- 
  tibble(
    steps=seq(0.01,0.99,0.01)
  ) %>% 
  mutate(
    likelihoods = dbinom(7,10,steps),
    priors_flat = dunif(steps)/sum(dunif(steps)),
    priors_fair = dbinom(5,10,steps)/sum(dbinom(5,10,steps)),
    posteriors_flat = (likelihoods*priors_flat)/marginal_likelihood,
    posteriors_fair=(likelihoods*priors_fair)/marginal_likelihood
)

ggplot(df,aes(steps,posteriors_flat)) + 
  geom_line(color='blue') + 
  xlab('p(heads)') + ylab('Posterior probability') +
  geom_line(aes(steps,posteriors_fair),color='red') +
  geom_line(aes(steps,priors_fair),linetype='dotted')

```

Note that the likelihood and marginal likelihood did not change - only the prior changed.  The effect of the change in prior to was to pull the posterior closer to the mass of the new prior, which is centered at 0.5.  

Now let's see what happens if we come to the analysis with an even stronger prior belief.  Let's say that instead of having previously observed 5 heads on 10 flips, we had flipped a coin 100 times and observed 50 flips.  This should in principle give us a much stronger prior, and as we see in Figure \@ref(fig:strongPrior), that's what happens: The prior is much more concentrated around 0.5, and the posterior is also much closer to the prior.  The general idea is that Bayesian inference combines the information from the prior and the likelihood, weighting the relative strength of each.


```{r strongPrior,fig.cap="Effects of the strength of the prior on the posterior distribution. The blue line shows the posterior obtained using the prior based on 5 heads out of 10 flips.  The dotted black line shows the prior based on 50 heasd out of 100 flips, and the red line shows the posterior based on that prior."}

# compute likelihoods for data under all values of p(heads).  here we use the quantized values from .01 to .99 in steps of 0.01

df <-
  df %>%
  mutate(
    priors_flips_strong = dbinom(50,100,steps)/sum(dbinom(50,100,steps)),
    posteriors_strongprior = (likelihoods*priors_flips_strong)/marginal_likelihood
  )



ggplot(df,aes(steps,posteriors_fair)) + 
  geom_line(color='blue') + 
  xlab('p(heads)') + ylab('Posterior probability') +
  geom_line(aes(steps,posteriors_strongprior),color='red') +
  geom_line(aes(steps,priors_flips_strong),linetype='dotted')

```

This example also highlights the sequential nature of Bayesian analysis -- the posterior from one analysis can become the prior for the next analysis.

Finally, it is important to realize that if the priors are strong enough, they can completely overwhelm the data.  Let's say that you have an absolute prior that $p_{heads}$ is between 0.2 and 0.3, such that you set the prior likelihood of all other values to zero.  What happens if we then compute the posterior?

```{r absolutePrior,fig.cap="Effects of the strength of the prior on the posterior distribution. The blue line shows the posterior obtained using the prior based on 5 heads out of 10 flips.  The dotted black line shows the prior based on 50 heasd out of 100 flips, and the red line shows the posterior based on that prior."}

# compute likelihoods for data under all values of p(heads).  here we use the quantized values from .01 to .99 in steps of 0.01

df <-
  df %>%
  mutate(
    priors_absolute = array(data=0,dim=length(steps))
  )
df$priors_absolute[which(df$steps<=0.3 & df$steps>=0.2)] <- 1
df$priors_absolute <- df$priors_absolute/sum(df$priors_absolute)
df <-
  df %>% 
  mutate(
    posteriors_absolute=(likelihoods*priors_absolute)/marginal_likelihood
  )

ggplot(df,aes(steps,posteriors_absolute)) + 
  geom_line(color='blue') + 
  xlab('p(heads)') + ylab('Posterior probability') +
  ylim(0,max(df$priors_absolute/10)*1.1) + 
  geom_line(aes(steps,priors_absolute/10),linetype='dotted')

```

In Figure \@ref(fig:absolutePrior) we see that there is zero density in the posterior for any of the values where the prior was set to zero - the data are overwhelmed by the absolute prior.

### Choosing a prior

The impact of priors on the resulting inferences are the most controversial aspect of Bayesian statistics.  There are various ways to choose one's priors, which (as we saw above) can impact the resulting inferences.  *Uninformative priors* attempt to bias the resulting posterior as little as possible, as we saw in the example of the uniform prior above.  It's also common to use *weakly informative priors*, which bias the result only very slightly. For example, if we had used a binomial distribution based on a one heads out of two coin flips, this would have been centered around 0.5 but fairly flat, biasing the posterior only slightly.  

It is also possible to use priors based on the scientific literature or pre-existing data, which we would call *empirical priors*.  In general, however, we will stick to the use of uninformative/weakly informative priors, since they raise the least concern about biasing our results.  In general it is a good idea to try any Bayesian analysis using multiple reasonable priors, and make sure that the results don't change in an important way based on the prior.

## Bayesian hypothesis testing

Having learned how to perform Bayesian estimation, we now turn to the use of Bayesian methods for hypothesis testing.  Let's say that there are two politicians who differ in their beliefs about support for the death penalty. Senator Smith thinks that only 40% of people support the death penalty, whereas Senator Jones thinks that 60% of people support the death penalty.  They arrange to have a poll done to test this, which asks 1000 randomly selected people whether they support the death penalty, which finds that 490 of people in the sample support the death penalty. Based on these data, we would like to know: Do the data support the claims of one senator over the other?  We can test this using a concept known as the [Bayes factor](https://bayesfactor.blogspot.com/2014/02/the-bayesfactor-package-this-blog-is.html).


### Bayes factors

The Bayes factor characterizes the relative likelihood of the data under two different hypotheses.  It is defined as:

$$
BF = \frac{p(data|H_1)}{p(data|H_2)}
$$
for two hypotheses $H_1$ and $H_2$.  In the case of our two senators, we know how to compute the likelihood of the data under each hypothesis using the binomial distribution.  We will put Senator Smith in the numerator and Senator Jones in the denominator, so that a value greater than one will reflect greater evidence for Senator Smith, and a value less than one will reflect greater evidence for Senator Jones.

```{r}
bf <- dbinom(490,1000,0.4)/dbinom(490,1000,0.6)
bf
```

This number provides a measure of the evidence that the data provides regarding the two hypotheses - in this case, it tells us the data support Senator Smith more than 3000 times more strongly than the support Senator Jones.

### Bayes factors for statistical hypotheses

In the previous example we had specific predictions from each senator, whose likelihood we could quantify using the binomial distribution. However, in real data analysis we generally must deal with uncertainty about our parameters, which complicates the Bayes factor.  However, in exchange the gain the ability to quantify the relative amount of evidence in favor of the null versus alternative hypotheses.  

Let's say that we are a medical researcher performing a clinical trial for the treatment of diabetes, and we wish to know whether a particular drug reduces blood glucose compared to placebo. We recruit a set of volunteers and randomly assign them to either drug or placebo group, and we measure the change in hemoglobin A1C (a marker for blood glucose levels) in each group over the period in which the drug or placebo was administered.  What we want to know is: Is there a difference between the drug and placebo?

First, let's generate some data and anlayze them using null hypothesis testing (see Figure \@ref(fig:bayesTesting)).

```{r bayesTesting,fig.cap="Box plots showing data for drug and placebo groups."}
set.seed(123456)
nsubs <- 40
effect_size <- 0.1
# randomize indiviuals to drug (1) or placebo (0)
drugDf <- 
  tibble(
    group=as.integer(runif(nsubs)>0.5)
  ) %>%
  mutate(
    hbchange=rnorm(nsubs) - group*effect_size
  )

ggplot(drugDf,aes(factor(group),hbchange)) +
  geom_boxplot() + 
  annotate('segment',x=0.5,xend=2.5,y=0,yend=0,linetype='dotted')
```

Let's perform an independent-samples t-test, which shows that there is a significant difference between the groups:

```{r}
drugTT <- t.test(hbchange~group,data=drugDf,alternative='greater')
drugTT
```

This test tells us that there is a significant difference between the groups, but it doesn't quantify how strongly the evidence supports the null versus alternative hypotheses.  To measure that, we can compute a Bayes factor using the BayesFactor package in R:

```{r}
bf_drug <- ttestBF(formula=hbchange~group,data=drugDf,
                  nullInterval = c(0,Inf))
bf_drug
```

The Bayes factor here tells us that the alternative hypothesis (i.e. that the difference is greater than zero) that is about 2.4 times more likely than the null hypothesis given the data.  This doesn't seem that strong, but how can we determine that?  There is a general guideline for interpretation of Bayes factors suggested by [Kass & Rafferty (1995)](https://www.andrew.cmu.edu/user/kk3n/simplicity/KassRaftery1995.pdf):

|BF|	Strength of evidence|
|---------|---------------------|
|1 to 3 |  not worth more than a bare mention|
|3 to 20| positive|
|20 to 150| strong|
|>150 | very strong|

Based on this, even though the statisical result is significant, the amount of evidence in favor of the alternative vs. the null hypothesis is weak enough that it's not worth even mentioning.  

Because the Bayes factor is comparing evidence for two hypotheses, it also allows us to assess whether there is evidence in favor of the null hypothesis, which we couldn't do with standard null hypothesis testing (because it already assumes that the null is true!).  This can be very useful to determine whether a non-significant result really provides strong evidence that there is no effect, or instead just reflects weak evidence overall.

## Suggested readings

- *The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy*, by Sharon Bertsch McGrayne
- *Doing Bayesian Data Analysis: A Tutorial Introduction with R*, by John K. Kruschke  
